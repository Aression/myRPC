# 2025-11-21 技术演进报告

## 概览
- 当前工作区相较上一版共 **53** 个文件发生变更，新增 **2,480** 行、删除 **6,764** 行，核心聚焦于“异步化 + 可观测 + 高可靠”三大方向。
- 关键主题：全链路异步 RPC 栈、序列化扩展（Kryo/RawMessage）、服务端 FastService 快路径与 LambdaMetafactory 调度、Redis + 文件双写持久化、统一配置与分环境日志、性能压测基线。
- 兼容性提示：同步客户端/服务端样例被移除，所有 Service API 全面切换至 `CompletableFuture`；旧的 `common.message.Rpc` 已替换为按类型编码的 `RawMessage`。

## 核心能力演进

### 协议与序列化层
- 引入 `common.message.RawMessage`，传输层延迟反序列化，只在业务线程根据 `messageType/serializerType` 解码，减少 IO 线程阻塞。
- 新增 `common.serializer.impl.KryoSerializer` 与 `ObjectSerializer` 注册到 `META-INF/services/common.serializer.Serializer`，实现多序列化策略动态加载，`SerializerFactory` 同步扩展工厂缓存。
- `pom.xml` 引入 Kryo 依赖、升级 Netty/Curator/Guava 版本，同时通过 `protobuf-maven-plugin` 自动生成消息类，构建链路支持 Java 21。

### 客户端演进
- `NettyRpcClient` 重写为纯异步模型：`sendRequest` 被弃用，仅暴露 `sendRequestAsync`（返回 `CompletableFuture<RpcResponse>`），引入连接管理器本地缓存 Channel，避免重复建连。
- 新增 `client.netty.UnprocessedRequests` 管理 requestId → Future 的映射，结合定时线程 (`SCHEDULER`) 做超时兜底；失败路径反向通知熔断器 `Breaker`。
- `NettyClientHandler` 只处理 `RawMessage`，把反序列化和业务完成回调放入 `clientWorkerPool`，避免在 IO 线程做重活，并对线程池做轻量监控。

### 服务端演进
- `NettyServerHandler` 区分 `FastService` 与普通服务：标注 `@FastService` 的实现（如 `EchoServiceImpl`）在 IO 线程内就地执行，其他请求封装进业务线程池，消除频繁上下文切换。
- 服务端方法调用由 `ServiceInvoker` 抽象，默认实现 `LambdaMetafactoryInvoker` 以 MethodHandle + invokeWithArguments 替换反射，压低反射开销并缓存 `Method`→`BiFunction`。
- RateLimit/Breaker 逻辑前置到 ServerHandler 异步流程，RPC 响应统一回写为 `RawMessage`，保持与客户端一致的序列化流程。

### 业务与数据一致性
- `UserService`/`UserServiceImpl` 现全部采用 `CompletableFuture<Result<...>>`，内部构建了 **BloomFilter + Caffeine + Redis** 的三层缓存体系，配套分布式互斥锁及布隆过滤器兜底穿透。
- 新增 `server.persistence.PeriodicPersistenceService`：通过 Redis 分布式锁确保多节点只有一个实例执行周期性落盘 (`JsonFileUtil`)；服务关闭时也会抢锁后落地，保证数据一致。
- `common.util.PerformanceTracker` 提供毫秒级阶段打点，可在客户端和服务端启用后获取全链路时序。

### 配置与可观测性
- `src/main/resources/application.properties` 新增序列化、限流、熔断、持久化开关等键值，默认选择 Kryo + 一致性哈希负载 + Configurable Token Bucket，方便通过系统属性覆盖。
- 引入 `logback.xml.echo` / `logback.xml.prod` 等多套日志模板，可按场景切换，主 `logback.xml` 也扩大了异步 Appender 配置。
- README 扩展到 v8/v9/v10（含一致性配置与高性能测试）阶段性说明，并给出了 Flight Recorder 采集命令。

### 测试与基准
- 移除旧的 `ConcurrentTestClient` / `TestServer` / 基础多线程示例，改为 `performance` 目录下一组自动化压测（Echo、RateLimit、Breaker、LoadBalance、RPC全链路），支持自动探测极限 QPS。
- 新增 `src/test/java/test` 与 `doc/11.16-11.20/` 等资料，用于沉淀阶段性测试报告与调优记录。

## 不兼容 & 风险提示
- **同步 API 下线**：客户端不再支持阻塞式 `sendRequest`，存量调用需迁移到 `CompletableFuture` 或在外层封装阻塞等待。
- **消息体变化**：旧 `common.message.Rpc` 删除，所有 Handler 均要求 `RawMessage`，第三方扩展 Encoder/Decoder 需同步修改。
- **序列化默认值变更**：`rpc.serializer.type` 默认切到 Kryo（type=3），如目标语言不支持需在配置中手动降级。
- **大规模重构**：`UserServiceImpl`/缓存链路加入大量 Redis/BloomFilter 依赖，部署环境若无 Redis 需确保 mock 或关闭相关逻辑。

## 建议验证
- 运行 `mvn test -pl performance -DskipSlow=false`，确认性能压测脚本可完整跑通，并输出 QPS 探测报告。
- 针对持久化能力，在多节点环境模拟锁竞争，观察 `PeriodicPersistenceService` 日志与 JSON 文件的最终一致性。
- 覆盖三种序列化（Object/JSON/Kryo）互通测试，确保 `SerializerFactory` SPI 注册完整。
- 启用 `PerformanceTracker.ENABLED=true`、`logback.xml.echo`，采集一次 `EchoPerformanceTest` + Flight Recorder，验证 tracing/metrics 联动。

