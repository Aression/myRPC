1. 学习了redis相关数据结构的用法和基本结构。重点：sortedSet底层通过跳表实现
2. jedis线程不安全，最好改成lettuce（单机多线程）、redisson（分布式多线程），或者Spring整合两者后的Spring Data Redis
3. 引入redission的时候mvn package报错，修改了原有的clientProxy构造函数，对传入的空值做了特别处理以规避NPE。
4. 为breaker的并发修改和访问增加新的测试；同时修改其他测试代码和原有实现。
  - 重构到junit5
  - 基础功能和并发访问测试分离
  - 修改限流器实现，全局使用纳秒作为时间间隔，直接传入tps参数而不是毫秒的时间间隔，直观且允许更高的tps
  - 低速率下限流精度的问题主要是由于sleep等系统调用的实际休眠时长并不是完全符合传入的时间的，在时间窗口比较小的情况下这些误差会引入更大的tps偏差；当扩大时间窗口后，相当于能得到更多的采样次数，短时间测试看到的是“瞬时噪声 + 启动偏差”，长期测试看到的是“真实平均速率”。本质上是一个离散采样误差问题，只能通过增加采样量解决。
  感受：测试划分和覆盖应该算是比较重要的一环。面测开的话应该重点准备。

5. 实现完善的性能测试
 - qps
 - P50/P95/P99

6. 高并发测试后的架构优化
 - 熔断器粒度不够细：高并发场景下，如果一个服务下的各个节点共用一个熔断器，则可能会在一些节点可用但部分节点不可用导致大量失败请求的时候屏蔽掉可用的节点，是否有必要转向对节点单独设计熔断器？已经细化。
 - 多节点的状态协同问题。之前是MultiNodeServer 在同一个 JVM 中启动了多个服务节点（端口 9991-9995），每个节点都创建了自己的 UserServiceImpl 实例。由于 userStore 是实例变量（private final Map），每个节点维护一份独立的内存数据。 当客户端负载均衡器将 Insert 请求分发到节点 A，而将随后的 Update 请求分发到节点 B 时，节点 B 的内存中没有该用户数据，从而导致 "User not found" 错误。解决方案：统一使用redis内的缓存作为当前可用的映射关系，服务下线的时候再本地化保存一遍快照，上线的时候直接使用redis里面的最新数据。
  - 改进：统一使用redis内的缓存作为当前可用的映射关系，服务下线的时候再本地化保存一遍快照，各个节点上线的时候先检查当前是否有其他存活节点，如果有则直接使用redis里面的最新数据，否则则使用本地保存的数据更新redis【todo：这里的存活验证以及唯一存活节点更新的逻辑没有实现】

7. 分布式场景下的数据一致性问题：在测试userIMPL的实际工作的时候，发现在多线程并发异步CURD的时候会出现比较严重的数据不一致问题。
  - 目前使用雪花算法生成全局唯一id来替换原有的顺序id生成，规避异步场景下并发生成大批冲突id导致插入大量失败的问题。

8. zkserviceregister设计模式的问题
  - 做压测的时候发现开的测试线程多了会导致zk挂掉无法注册服务，发现是因为注册器是每个节点单独的，导致线程数量太多的话：
    - ZK客户端连接过多: 5个服务节点各自创建独立的ZK客户端连接,并发启动导致ZK服务端压力过大
    - 注册失败被静默忽略: ZKServiceRegister.register() 只记录日志不抛异常,没有重试机制
    - 注册时序混乱: 服务在端口监听前就注册到ZK,且测试代码不检查ZK注册是否成功
    - sessionTimeoutMs过长: 40秒的会话超时在高并发时占用过多资源
    - 等待时间不足: 测试只等待15秒,线程数100时服务可能未完全注册就开始调用

  - 解决方案：注册器改成单例，调整参数，添加注册验证和重试逻辑。[没改，因为实际上感觉差别不大，也就5个线程，不可能开很多。不过设计模式上来说应该改。]

9. 持久化的并发写保护
  - 压测时发现只有第一次能成功，第二次会失败，因为写进去的东西没有意义，因为并发访问不符合json格式。
  - 修改：修改持久化策略，用redis实现分布式锁来让各个节点竞争持久化的机会；同时为server添加定时持久化机制。
    ```java
    // 健壮的分布式锁，和requestID无关。

    public static boolean tryGetDistributedLock(String lockKey, int leaseTimeMillis) {
        try {
            RLock lock = getClient().getLock(lockKey);
            // 直接尝试获取锁，Redisson会用当前线程ID作为所有者标识
            return lock.tryLock(0, leaseTimeMillis, TimeUnit.MILLISECONDS);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            return false;
        } catch (Exception e) {
            logger.error("获取分布式锁失败: lockKey={}", lockKey, e);
            return false;
        }
    }

    public static boolean releaseDistributedLock(String lockKey) {
        try {
            RLock lock = getClient().getLock(lockKey);
            // 只有持有锁的那个线程调用这个方法才能成功
            // 如果锁不存在或不属于当前线程，unlock会抛出异常或什么都不做
            if (lock.isLocked() && lock.isHeldByCurrentThread()) {
                lock.unlock();
                return true;
            }
            return false;
        } catch (Exception e) {
            logger.error("释放分布式锁失败: lockKey={}", lockKey, e);
            return false;
        }
    }
    ```

10. 项目架构调整和职责分离
  - 原有的架构下，已经有了nettyserver后又包装了一个multinodeserver，没有必要且引入了不必要的耦合。现在已经删除并重新注册。.
  - 序列化和反序列化操作不应该直接在 Netty 的 I/O 线程（事件循环）执行的 handler 责任链中完成，而应该在 NettyServerHandler（或其他类似的处理程序）中将任务提交到独立的业务线程池来执行。
    原有的代码把它直接放在了netty的handler里面执行，对io资源消耗过高。

11. 异步架构优化
  起初是发现优化handler后qps仍然很低，通过jfr日志导出：
  ```cmd
  $env:MAVEN_OPTS="-XX:+FlightRecorder -XX:StartFlightRecording=duration=60s,filename=e:\CODES\myRPC\echo_async_jdk21.jfr,settings=profile,stackdepth=1024"; mvn test -Dtest=EchoPerformanceTest#testEchoPerformance
  ```
  分析发现问题出在文件io，调整记录栈深度后发现问题是被派生的测试 JVM 进程中的测试代码 EchoPerformanceTest#testEchoPerformance 在两个事件发送之间存在较长的空闲或处理时间。

  进一步分析发现原来是jdk版本管理的问题:
    java.lang.invoke 包的内部实现在 Java 11 → 21 之间有重大变化
    Maven Surefire fork 的测试子进程使用 Java 21 运行，但加载的是用 Java 11 编译的字节码
    LambdaMetafactory.metafactory() 动态生成的字节码与 Java 21 的内部 API 不兼容
    导致 NoSuchMethodError 和 IncompatibleClassChangeError
  
  修改后发现logback版本不对，mvn dependency:tree -Dverbose 发现是zk引入的logback覆盖了显式引入的新版本。添加exclusion解决。

  再往下看发现是surefire插件的bug，但是提升到3.5.4之后仍然存在一堆报错，时间也没有很好的改善。

  最后发现是序列化方式在client侧的缓存没有正常生效，成功解决了速度过慢的问题：
  
  - 序列化器缓存 - 修改SerializerFactory添加ConcurrentHashMap缓存，避免每次请求都重新查找序列化器
  - 反射替换 - 实现ServiceInvoker + LambdaMetafactoryInvoker，使用MethodHandle替代反射
  - 线程模型优化 - 添加@FastService注解，让EchoService直接在IO线程执行
  - Kryo序列化 - 添加高性能Kryo序列化器
  
  优化后能达到86931.65的峰值QPS,此时的并发度为500,5个server。